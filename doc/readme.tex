\documentclass[11pt,a4paper]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}

\usepackage{minted}
\newminted{py}{%
%		linenos,
		fontsize=\footnotesize,
		tabsize=2,
		mathescape,
}
\newminted{text}{%
%		linenos,
		fontsize=\footnotesize,
		tabsize=2,
		mathescape,
}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\title{Approximated PCA}
\author{Rodrigo Arias Mallo}

\begin{document}
\maketitle

\section{Introduction}

The PCA method transforms a set of observations of possibly correlated variables 
into a set of values of linearly uncorrelated variables called principal 
components using an orthogonal transformation (rotations and reflections).

The transformation, projects the data in a new subspace, in which each new 
variable it's now uncorrelated. That means that the covariance of each pair of 
new variables is zero.
%
To compute the transformation, different approaches can be taken. In a first 
attempt, the covariance matrix will be used.
%
Let $x_{ij}$ be the observation $j$ of the variable $i$. Let $n$ be the number 
of variables and $m$ the number of observations.
%
Each element $s_{ij}$ in the covariance matrix $S$ is computed by
%
$$ s_{ij} = \frac{\sum x_{ik} x_{jk} - \sum x_{ij} x_{jk}}{n (n-1)} $$
%
Once the covariance matrix $S$ in computed, it can be used to find his 
eigenvalues and eigenvectors. One method to compute those values, is a 
combination of a Householder transformation, followed by the QR transformation.  
The first will transform $S$ in a product of two matrix $Q$ and $R$.
%
	$$ S = QR $$
%
Such that $R$ contains 3 diagonals (tridiagonal) with elements and zeros in the 
rest. The QR transformation then takes this two matrices, and computes 
iteratively a new diagonal matrix $A^{(i+1)} = R^{(i)} Q^{(i)}$. Finally, the 
eigenvalues are in the diagonal of $A$ and the eigenvectors are computed from 
these.

\section{Householder tridiagonalization}

The Householder tridiagonalization it's a process where a matrix $A$ is 
transformed by multiplying with an orthogonal matrix $P^{(k)}$:
%
$P^{(k)} = I - 2ww^T$
%
Such matrix $P^{(k)}$ has been prepared, so that $P^{(k)}A$ is a new matrix, 
with zeros below the $k+1$ element in the $k$ column. This new matrix, has the 
same eigenvalues as the provious $A$. The step is repeated until the final 
matrix has only elements in the diagonal, and the two sub-diagonals. The process 
is similar to a Gaussian elimination.

\section{Eigenvalue sensitivity}

% Extracted from Golub, Van Loan - Matrix multiplications.

Corolary 8.1.6: If $A$ and $A+E$ are \textit{n-by-n} symmetric matrices, then
%
$$ |\lambda_k(A+E) - \lambda_k(A)| \le \norm{E}_2 $$
%
for $k = 1:n$.

Then, the difference between the eigenvalue of a noisy matrix, and the original, 
can be bounded by the 2-norm of $E$, also the maximum eigenvalue of $E$.




\end{document}
